{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-1\n",
    "###  Lode modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib # to read nii files\n",
    "import shutil # for file operations\n",
    "import glob  # use to make file list from diectory\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageFilter\n",
    "import cv2 # opencv library\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score,recall_score\n",
    "from skimage import filters\n",
    "from skimage import measure\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.morphology import closing, square\n",
    "import traceback\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ['Alzheimer',\"MCI\", 'Normal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-2\n",
    "### Converting Nii files into numpy 3-D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Alzheimer...\n",
      "working on A25_018_S_5074.nii...\n",
      "working on A27_018_S_4696.nii...\n",
      "working on A78_019_S_4549.nii...\n",
      "working on A15_100_S_5106.nii...\n",
      "working on A41_130_S_4982.nii...\n",
      "working on A48_130_S_4984.nii...\n",
      "working on A16_053_S_5208.nii...\n",
      "working on A1_136_S_4993.nii...\n",
      "working on A57_130_S_5006.nii...\n",
      "working on A20_019_S_5012.nii...\n",
      "working on A19_019_S_5019.nii...\n",
      "working on A93_130_S_4589.nii...\n",
      "working on A33_006_S_4153.nii...\n",
      "working on A58_006_S_4867.nii...\n",
      "working on A29_010_S_5163.nii...\n",
      "working on A34_002_S_5018.nii...\n",
      "working on A90_053_S_5070.nii...\n",
      "working on A26_018_S_4733.nii...\n",
      "working on A98_130_S_4660.nii...\n",
      "working on A53_006_S_4546.nii...\n",
      "working on A63_013_S_5071.nii...\n",
      "working on A74_019_S_4477.nii...\n",
      "working on A55_130_S_4990.nii...\n",
      "working on A3_130_S_5231.nii...\n",
      "working on A95_130_S_4641.nii...\n",
      "working on A72_130_S_5059.nii...\n",
      "working on A104_130_S_4971.nii...\n",
      "working on A11_130_S_4730.nii...\n",
      "working on A18_031_S_4024.nii...\n",
      "working on A45_006_S_4192.nii...\n",
      "working on A6_130_S_4997.nii...\n",
      "working on A23_019_S_4252.nii...\n",
      "working on A2_131_S_5138.nii...\n",
      "working on A24_018_S_5240.nii...\n",
      "working on MCI...\n",
      "working on M37_053_S_2396.nii...\n",
      "working on M110_006_S_4363.nii...\n",
      "working on M50_002_S_0729.nii...\n",
      "working on M176_013_S_4395.nii...\n",
      "working on M33_130_S_2373.nii...\n",
      "working on M2_012_S_4012.nii...\n",
      "working on M324_130_S_4294.nii...\n",
      "working on M4_006_S_4960.nii...\n",
      "working on M320_053_S_4661.nii...\n",
      "working on M338_129_S_4073.nii...\n",
      "working on M42_031_S_2017.nii...\n",
      "working on M25_002_S_1155.nii...\n",
      "working on M217_130_S_4468.nii...\n",
      "working on M128_130_S_4925.nii...\n",
      "working on M211_018_S_4597.nii...\n",
      "working on M8_006_S_4363.nii...\n",
      "working on M19_002_S_4219.nii...\n",
      "working on M292_031_S_4590.nii...\n",
      "working on M30_130_S_2403.nii...\n",
      "working on M222_018_S_4889.nii...\n",
      "working on M20_002_S_4171.nii...\n",
      "working on M44_018_S_2155.nii...\n",
      "working on M46_018_S_2133.nii...\n",
      "working on M16_002_S_4251.nii...\n",
      "working on M153_012_S_4849.nii...\n",
      "working on M28_136_S_4848.nii...\n",
      "working on M41_031_S_2018.nii...\n",
      "working on M337_100_S_4970.nii...\n",
      "working on M106_136_S_4517.nii...\n",
      "working on M328_100_S_4512.nii...\n",
      "working on M63_002_S_2073.nii...\n",
      "working on M258_031_S_4005.nii...\n",
      "working on M296_031_S_4721.nii...\n",
      "working on M283_031_S_4203.nii...\n",
      "working on M6_006_S_4679.nii...\n",
      "working on M11_002_S_4746.nii...\n",
      "working on M22_002_S_2043.nii...\n",
      "working on M118_136_S_0107.nii...\n",
      "working on M17_002_S_4237.nii...\n",
      "working on M13_002_S_4521.nii...\n",
      "working on M15_002_S_4447.nii...\n",
      "working on M102_006_S_4346.nii...\n",
      "working on M340_129_S_4287.nii...\n",
      "working on M172_013_S_4268.nii...\n",
      "working on M231_019_S_4293.nii...\n",
      "working on M331_100_S_4512.nii...\n",
      "working on M26_136_S_4836.nii...\n",
      "working on M139_012_S_4094.nii...\n",
      "working on M40_031_S_2022.nii...\n",
      "working on M236_019_S_4548.nii...\n",
      "working on M273_031_S_4149.nii...\n",
      "working on M183_130_S_4542.nii...\n",
      "working on M175_130_S_4817.nii...\n",
      "working on M149_012_S_4188.nii...\n",
      "working on M339_129_S_4220.nii...\n",
      "working on M35_136_S_4956.nii...\n",
      "working on M127_006_S_4713.nii...\n",
      "working on M300_130_S_4405.nii...\n",
      "working on M288_031_S_4476.nii...\n",
      "working on M23_002_S_2010.nii...\n",
      "working on M12_002_S_4654.nii...\n",
      "working on M161_013_S_1186.nii...\n",
      "working on M305_041_S_5026.nii...\n",
      "working on M38_053_S_2357.nii...\n",
      "working on M108_136_S_4408.nii...\n",
      "working on M29_136_S_4932.nii...\n",
      "working on M10_002_S_4799.nii...\n",
      "working on M171_013_S_4236.nii...\n",
      "working on M24_002_S_1268.nii...\n",
      "working on M336_100_S_4884.nii...\n",
      "working on M113_136_S_4189.nii...\n",
      "working on M268_031_S_4042.nii...\n",
      "working on M306_053_S_0919.nii...\n",
      "working on M185_013_S_4791.nii...\n",
      "working on M144_012_S_4128.nii...\n",
      "working on M162_130_S_4883.nii...\n",
      "working on M7_006_S_4515.nii...\n",
      "working on M52_002_S_0729.nii...\n",
      "working on M180_130_S_4605.nii...\n",
      "working on M27_013_S_2324.nii...\n",
      "working on M1_013_S_2389.nii...\n",
      "working on M242_130_S_4417.nii...\n",
      "working on M43_018_S_2180.nii...\n",
      "working on M280_130_S_4415.nii...\n",
      "working on M18_002_S_4229.nii...\n",
      "working on M332_100_S_4556.nii...\n",
      "working on M14_002_S_4473.nii...\n",
      "working on M301_031_S_4947.nii...\n",
      "working on M263_031_S_4029.nii...\n",
      "working on M39_031_S_2233.nii...\n",
      "working on M316_053_S_4557.nii...\n",
      "working on M192_013_S_4985.nii...\n",
      "working on M325_053_S_4813.nii...\n",
      "working on M45_018_S_2138.nii...\n",
      "working on M345_130_S_2391.nii...\n",
      "working on M214_018_S_4809.nii...\n",
      "working on M226_019_S_4285.nii...\n",
      "working on M3_010_S_4135.nii...\n",
      "working on M34_100_S_2351.nii...\n",
      "working on M157_012_S_4987.nii...\n",
      "working on M32_130_S_4250.nii...\n",
      "working on M31_130_S_2402.nii...\n",
      "working on M188_013_S_4917.nii...\n",
      "working on M218_018_S_4868.nii...\n",
      "working on Normal...\n",
      "working on N77_002_S_4270.nii...\n",
      "working on N4_136_S_0186.nii...\n",
      "working on N28_002_S_4213.nii...\n",
      "working on N139_018_S_4257.nii...\n",
      "working on N128_013_S_4579.nii...\n",
      "working on N158_019_S_4367.nii...\n",
      "working on N56_002_S_0685.nii...\n",
      "working on N17_012_S_4643.nii...\n",
      "working on N90_006_S_4357.nii...\n",
      "working on N30_010_S_4345.nii...\n",
      "working on N39_031_S_4474.nii...\n",
      "working on N117_031_S_4496.nii...\n",
      "working on N138_013_S_4731.nii...\n",
      "working on N14_006_S_4150.nii...\n",
      "working on N54_002_S_0413.nii...\n",
      "working on N69_130_S_4343.nii...\n",
      "working on N81_006_S_0731.nii...\n",
      "working on N135_013_S_4616.nii...\n",
      "working on N101_053_S_4578.nii...\n",
      "working on N11_100_S_5246.nii...\n",
      "working on N3_136_S_4269.nii...\n",
      "working on N13_100_S_4469.nii...\n",
      "working on N8_129_S_4371.nii...\n",
      "working on N141_018_S_4257.nii...\n",
      "working on N38_031_S_4218.nii...\n",
      "working on N59_002_S_1261.nii...\n",
      "working on N2_136_S_4433.nii...\n",
      "in last except block\n",
      "N2_136_S_4433.nii\n",
      "working on N168_031_S_4032.nii...\n",
      "working on N7_129_S_4396.nii...\n",
      "working on N122_012_S_4545.nii...\n",
      "working on N76_002_S_4264.nii...\n",
      "working on N50_136_S_4726.nii...\n",
      "working on N114_010_S_4442.nii...\n",
      "working on N35_018_S_4400.nii...\n",
      "working on N42_130_S_4352.nii...\n",
      "working on N6_129_S_4422.nii...\n",
      "working on N20_002_S_4225.nii...\n",
      "working on N149_18_S_4399.nii...\n",
      "working on N132_013_S_4580.nii...\n",
      "working on N119_012_S_4026.nii...\n",
      "working on N29_006_S_4485.nii...\n",
      "working on N165_031_S_4021.nii...\n",
      "working on N145_018_S_4349.nii...\n",
      "working on N162_019_S_4367.nii...\n",
      "working on N10_129_S_0778.nii...\n",
      "working on N51_002_S_0295.nii...\n",
      "working on N12_100_S_4511.nii...\n",
      "working on N26_006_S_4449.nii...\n",
      "working on N1_136_S_4727.nii...\n",
      "working on N72_002_S_4262.nii...\n",
      "working on N86_129_S_4369.nii...\n",
      "working on N27_002_S_1280.nii...\n",
      "('3d array shape : ', (64, 64, 6720))\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "# Declaring the home variables that will be used throughout the script.\n",
    "\n",
    "home_files_dir = '/home/ubuntu/Select_original_fmri/'\n",
    "#output_dir='/home/ubuntu/final_src/DeepNeuralnets--Alzheimer/videoclassification/'\n",
    "\n",
    "#removing existing \"output_array\" (old data) folder from the directory '/home/ubuntu/Select_original_fmri/images'\n",
    "try:\n",
    "    os.mkdir('output_array')\n",
    "except:\n",
    "    shutil.rmtree('output_array')\n",
    "    os.mkdir('output_array')\n",
    "# iterating /home/ubuntu/Select_original_fmri/niifiles/class_ (Alzheimer, MCI , Normal)\n",
    "for class_ in classes:\n",
    "    print ('working on ' + class_ + '...')\n",
    "    \n",
    "    for root, dir ,files in os.walk(os.path.join(home_files_dir+\"niifiles/\" , class_)): \n",
    "        \n",
    "        # making class folder inside the '/home/ubuntu/Select_original_fmri/images' directory\n",
    "        try:\n",
    "            #print('test1')\n",
    "            os.mkdir( 'output_array' +'/'+ class_ + '/')\n",
    "            \n",
    "        except:\n",
    "            #print('caught!!!')\n",
    "            pass\n",
    "        # iterating through each nii file in each class in the '/home/ubuntu/Select_original_fmri/niifiles' directory\n",
    "        count = 0\n",
    "        for file_ in files:\n",
    "            print 'working on ' + file_ + '...'\n",
    "            try:\n",
    "                # extracting data from nii files\n",
    "                x = nib.load(os.path.join(home_files_dir+\"niifiles/\" , class_) + '/' + file_).get_data()\n",
    "                #print(\"x_shape\",x.shape)\n",
    "                x_arr=np.array(x)\n",
    "                #print (\"x_arr shape : \",x_arr.shape)\n",
    "                #converting X_arr into 3_D array.........\n",
    "                x_3d = x_arr.reshape(x_arr.shape[0],x_arr.shape[1],x_arr.shape[2]*x_arr.shape[3])\n",
    "                np.save('output_array' +'/'+ class_ + '/'+str(file_)+str(count),x_3d)   \n",
    "                count+=1\n",
    "                #print(\"3d array shape : \",x_3d.shape)\n",
    "\n",
    "            except:\n",
    "                print(\"in last except block\")\n",
    "                print file_\n",
    "print(\"3d array shape : \",x_3d.shape)\n",
    "print('done..')\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-3\n",
    "### Prepare Data for training\n",
    "#### Dividing data into test and training sets and making output labels for corresponding classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on Alzheimer...\n",
      "('root: ', 'output_array/Alzheimer')\n",
      "working on MCI...\n",
      "('root: ', 'output_array/MCI')\n",
      "working on Normal...\n",
      "('root: ', 'output_array/Normal')\n",
      "('X train shape: ', (40,))\n",
      "('Y label shape: ', (40,))\n",
      "('X test shape: ', (16,))\n",
      "('Y test label: ', (16,))\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_list=[]\n",
    "test_list=[]\n",
    "file_list=[]\n",
    "\n",
    "for class_ in classes:\n",
    "    print ('working on ' + class_ + '...')\n",
    "    \n",
    "    for root, dir ,files in os.walk(os.path.join('output_array', class_)): \n",
    "        length=len(files)\n",
    "        print(\"root: \",root)\n",
    "        if class_ == 'Alzheimer': \n",
    "            for file_ in files:            \n",
    "                # add array and label\n",
    "                file_list.append((np.load(root+'/'+file_),0))\n",
    "            #image data division\n",
    "            test_list=file_list[:int(length*0.3)]\n",
    "            train_list=file_list[len(test_list):]\n",
    "                 \n",
    "                \n",
    "        elif class_ == 'MCI':\n",
    "            len2=len(file_list)\n",
    "            for file_ in files:            \n",
    "                \n",
    "                file_list.append((np.load(root+'/'+file_),1))\n",
    "            #image  data  diision\n",
    "            test_list +=file_list[len2:int(len2+length*0.3)]\n",
    "            train_list += file_list[int(len2+length*0.3):]\n",
    "           \n",
    "            \n",
    "        # for Normal Class\n",
    "        \n",
    "        else:\n",
    "            len3=len(file_list)\n",
    "            for file_ in files:            \n",
    "        \n",
    "                file_list.append((np.load(root+'/'+file_),2))\n",
    "            #image  data  diision\n",
    "            test_list += file_list[len3:int(len3+length*0.3)]\n",
    "            train_list += file_list[int(len3+length*0.3):]\n",
    "            \n",
    "#print (\"length of train list: \",len(train_list))\n",
    "#print(\"length of train labels:\",len(train_labels))\n",
    "#print(\"length of test list:\",len(test_list))\n",
    "#print(\"length of test labels:\",len(test_labels))\n",
    "np.random.shuffle(train_list)\n",
    "np.random.shuffle(test_list)\n",
    "X_train,Y_train=zip(*train_list)\n",
    "X_test,Y_test=zip(*test_list)\n",
    "\n",
    "X_train=np.array(X_train)\n",
    "Y_train=np.array(Y_train)\n",
    "X_test=np.array(X_test)\n",
    "Y_test=np.array(Y_test)\n",
    "\n",
    "print(\"X train shape: \",X_train.shape)\n",
    "print(\"Y label shape: \",Y_train.shape)\n",
    "print(\"X test shape: \",X_test.shape)\n",
    "print(\"Y test label: \",Y_test.shape)\n",
    "print('done...')\n",
    "       \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-4\n",
    "## Train \n",
    "#### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: libcublas.so.7.5: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a649b509054f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: libcublas.so.7.5: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: libcublas.so.7.5: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a17ffc06526b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 41, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\nImportError: libcublas.so.7.5: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/install_sources#common_installation_problems\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.applications import VGG16\n",
    "import keras\n",
    "#from keras import utils as np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-4\n",
    "### set requied parameters for model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'to_categorical'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9f4bc1403726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mY_train\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#np.random.shuffle(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'to_categorical'"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "input_shape = [None,None,140]\n",
    "Y_train= np_utils.to_categorical(Y_train, num_classes=3)\n",
    "Y_test= np_utils.to_categorical(Y_test, num_classes=3)\n",
    "#np.random.shuffle(data)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Step-5\n",
    "### Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, None, None, 32)    40352     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, None, None, 64)    18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 461,155\n",
      "Trainable params: 461,155\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Conv2D(128, (3, 3)))\n",
    "model.add(Conv2D(256, (3, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-6\n",
    "### Complie Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-7\n",
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22 samples, validate on 9 samples\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 4s 190ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 4s 190ms/step - loss: 10.2570 - acc: 0.3636 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 4s 191ms/step - loss: 10.2570 - acc: 0.3636 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 4s 200ms/step - loss: 10.5219 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 5s 233ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 6s 261ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 5s 231ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 6s 253ms/step - loss: 10.2570 - acc: 0.3636 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 6s 257ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 6s 253ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 5s 243ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 5s 249ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 5s 234ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 6s 263ms/step - loss: 10.2570 - acc: 0.3636 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 6s 252ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 5s 224ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 5s 227ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 5s 242ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 5s 225ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 5s 226ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 5s 216ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 5s 219ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 5s 222ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 35/100\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 36/100\n",
      "22/22 [==============================] - 5s 238ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 37/100\n",
      "22/22 [==============================] - 5s 223ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 38/100\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 39/100\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 40/100\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 41/100\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 42/100\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 43/100\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 44/100\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 45/100\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 10.2570 - acc: 0.3636 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 46/100\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 47/100\n",
      "22/22 [==============================] - 5s 220ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 48/100\n",
      "22/22 [==============================] - 5s 221ms/step - loss: 11.7223 - acc: 0.2727 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 49/100\n",
      "22/22 [==============================] - 5s 228ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 50/100\n",
      "22/22 [==============================] - 6s 256ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 51/100\n",
      "22/22 [==============================] - 6s 265ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 52/100\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 53/100\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 54/100\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 55/100\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 56/100\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 57/100\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 58/100\n",
      "22/22 [==============================] - 5s 243ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 59/100\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 60/100\n",
      "22/22 [==============================] - 5s 243ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "22/22 [==============================] - 4s 199ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 62/100\n",
      "22/22 [==============================] - 4s 190ms/step - loss: 11.5148 - acc: 0.2727 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 63/100\n",
      "22/22 [==============================] - 4s 189ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 64/100\n",
      "22/22 [==============================] - 4s 190ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 65/100\n",
      "22/22 [==============================] - 4s 202ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 66/100\n",
      "22/22 [==============================] - 4s 193ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 67/100\n",
      "22/22 [==============================] - 4s 193ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 68/100\n",
      "22/22 [==============================] - 4s 195ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 69/100\n",
      "22/22 [==============================] - 4s 198ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 70/100\n",
      "22/22 [==============================] - 4s 195ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 71/100\n",
      "22/22 [==============================] - 4s 195ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 72/100\n",
      "22/22 [==============================] - 4s 195ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 73/100\n",
      "22/22 [==============================] - 4s 194ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 74/100\n",
      "22/22 [==============================] - 4s 200ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 75/100\n",
      "22/22 [==============================] - 4s 197ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 76/100\n",
      "22/22 [==============================] - 5s 248ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 77/100\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 78/100\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 79/100\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 80/100\n",
      "22/22 [==============================] - 5s 250ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 81/100\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 82/100\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 83/100\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 84/100\n",
      "22/22 [==============================] - 5s 246ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 85/100\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 86/100\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 87/100\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 88/100\n",
      "22/22 [==============================] - 6s 257ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 89/100\n",
      "22/22 [==============================] - 5s 242ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 90/100\n",
      "22/22 [==============================] - 5s 240ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 91/100\n",
      "22/22 [==============================] - 5s 242ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 92/100\n",
      "22/22 [==============================] - 5s 242ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 93/100\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 94/100\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 95/100\n",
      "22/22 [==============================] - 5s 241ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 96/100\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 97/100\n",
      "22/22 [==============================] - 5s 248ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 98/100\n",
      "22/22 [==============================] - 5s 244ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 99/100\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n",
      "Epoch 100/100\n",
      "22/22 [==============================] - 5s 245ms/step - loss: 10.9896 - acc: 0.3182 - val_loss: 10.7454 - val_acc: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7aa9b2fe90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=100,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-8\n",
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test loss:', 10.745396614074707)\n",
      "('Test accuracy:', 0.3333333432674408)\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
