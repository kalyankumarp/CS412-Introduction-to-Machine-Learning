{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "y-moriGADdaj",
    "outputId": "1cbdac51-f3c6-47fe-e52f-1db3b090a206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-YBCrmoinmCl",
    "outputId": "99206f2e-b5c4-4e97-aded-6d8e3fdb86a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/IML PRoject\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/IML PRoject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zDXPAEwoLnB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbN5skodoNzY"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.layers import MaxPool3D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import AveragePooling3D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZM1nchJoSdP"
   },
   "outputs": [],
   "source": [
    "data_path = 'tfrecords_data'\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "871hMI0noYb5"
   },
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "    \"\"\"\n",
    "    Parses an image and label from the given `serialized_example`.\n",
    "    It is used as a map function for `dataset.map`\n",
    "    \"\"\"\n",
    "\n",
    "    features = tf.compat.v1.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'img_channels': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
    "            'img_height': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
    "            'img_width': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
    "            'img_raw': tf.compat.v1.FixedLenFeature([], tf.string),\n",
    "            'sex': tf.compat.v1.FixedLenFeature([], tf.string),\n",
    "            'age': tf.compat.v1.FixedLenFeature([], tf.float32),\n",
    "            'label': tf.compat.v1.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "    \n",
    "    \n",
    "    \n",
    "    height = tf.cast(features['img_height'], tf.int32)\n",
    "    width = tf.cast(features['img_width'], tf.int32)\n",
    "    channels = tf.cast(features['img_channels'], tf.int32)\n",
    "    image = tf.compat.v1.decode_raw(features['img_raw'], tf.float64)\n",
    "    image = tf.reshape(image, (channels, height, width, 1))\n",
    "    label = tf.compat.v1.decode_raw(features['label'], tf.float64)\n",
    "    sex = tf.cast(features['sex'], tf.string)\n",
    "    age = tf.cast(features['age'], tf.float32)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7BUAjNIohmJ"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(os.path.join(data_path, 'train.tfrecords'), compression_type='GZIP').map(decode).batch(batch_size)\n",
    "val_dataset = tf.data.TFRecordDataset(os.path.join(data_path, 'validation.tfrecords'), compression_type='GZIP').map(decode).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpKfLlFcoq0e"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "    he_normal = tf.keras.initializers.he_normal(seed=0)\n",
    "    lecun_normal = tf.keras.initializers.lecun_normal(seed=0)\n",
    "\n",
    "    input_3d = (62, 96, 96, 1)\n",
    "    pool_3d = (2, 2, 2)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=input_3d))\n",
    "    model.add(Conv3D(filters=8,\n",
    "                     kernel_size=3,\n",
    "                     kernel_regularizer=kernel_regularizer,\n",
    "                     kernel_initializer=he_normal))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool3D(pool_size=pool_3d, name='pool1'))\n",
    "\n",
    "    model.add(Conv3D(filters=8,\n",
    "                     kernel_size=3,\n",
    "                     kernel_regularizer=kernel_regularizer,\n",
    "                     kernel_initializer=he_normal))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool3D(pool_size=pool_3d, name='pool2'))\n",
    "\n",
    "    model.add(Conv3D(filters=8,\n",
    "                     kernel_size=3,\n",
    "                     kernel_regularizer=kernel_regularizer,\n",
    "                     kernel_initializer=he_normal))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(MaxPool3D(pool_size=pool_3d, name='pool3'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, name='dense1', kernel_regularizer=kernel_regularizer, kernel_initializer=he_normal))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.5, name='dropout1'))\n",
    "\n",
    "    model.add(Dense(512, activation='relu', name='dense2', kernel_regularizer=kernel_regularizer, kernel_initializer=he_normal))\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.5, name='dropout2'))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax', name='softmax', kernel_initializer=lecun_normal))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6q2V_YlToxFg"
   },
   "outputs": [],
   "source": [
    "reducelr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy',\n",
    "                                                         factor=0.2,\n",
    "                                                         patience=5,\n",
    "                                                         min_delta=0.01,\n",
    "                                                         verbose=1)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(os.path.join('checkpoints', 'cp.ckpt'),\n",
    "                                                         monitor='val_categorical_accuracy',\n",
    "                                                         verbose=1,\n",
    "                                                         save_best_only=True,\n",
    "                                                         mode='max')\n",
    "weights_callback = tf.keras.callbacks.ModelCheckpoint(os.path.join('weights', 'best_weights.h5'),\n",
    "                                                      monitor='val_categorical_accuracy',\n",
    "                                                      verbose=1,\n",
    "                                                      save_best_only=True,\n",
    "                                                      save_weights_only=True,\n",
    "                                                      mode='max')\n",
    "\n",
    "callbacks_list = [weights_callback, checkpoint_callback, reducelr_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 823
    },
    "colab_type": "code",
    "id": "XceAcvCXo05H",
    "outputId": "e286eab5-236e-48c3-9668-a9df89a6a7e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d (Conv3D)              (None, 60, 94, 94, 8)     224       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 60, 94, 94, 8)     32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 60, 94, 94, 8)     0         \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling3D)         (None, 30, 47, 47, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 28, 45, 45, 8)     1736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 45, 45, 8)     32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 28, 45, 45, 8)     0         \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling3D)         (None, 14, 22, 22, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 12, 20, 20, 8)     1736      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 12, 20, 20, 8)     32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 12, 20, 20, 8)     0         \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling3D)         (None, 6, 10, 10, 8)      0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4800)              0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 1024)              4916224   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout1 (Dropout)           (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense2 (Dense)               (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 5,446,355\n",
      "Trainable params: 5,446,307\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CU9OWXtbQ0gH"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join('checkpoints', 'cp.ckpt')):\n",
    "    model = load_model(os.path.join('checkpoints', 'cp.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPmQA4neabp"
   },
   "outputs": [],
   "source": [
    "# 8.0000e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0Xwc9JhLo3k6",
    "outputId": "5d0f9e4b-4f23-475a-acac-1845c3fe19e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "     66/Unknown - 68s 1s/step - loss: 3.1814 - categorical_accuracy: 0.9175\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.76552, saving model to weights/best_weights.h5\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.76552, saving model to checkpoints/cp.ckpt\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: checkpoints/cp.ckpt/assets\n",
      "66/66 [==============================] - 91s 1s/step - loss: 3.1814 - categorical_accuracy: 0.9175 - val_loss: 3.5583 - val_categorical_accuracy: 0.7655 - lr: 4.0000e-07\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.1966 - categorical_accuracy: 0.9180\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.76552\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy did not improve from 0.76552\n",
      "66/66 [==============================] - 127s 2s/step - loss: 3.1966 - categorical_accuracy: 0.9180 - val_loss: 3.5603 - val_categorical_accuracy: 0.7609 - lr: 4.0000e-07\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.1908 - categorical_accuracy: 0.9165\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.76552\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy did not improve from 0.76552\n",
      "66/66 [==============================] - 126s 2s/step - loss: 3.1908 - categorical_accuracy: 0.9165 - val_loss: 3.5623 - val_categorical_accuracy: 0.7540 - lr: 4.0000e-07\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.1833 - categorical_accuracy: 0.9232\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.76552\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.76552\n",
      "66/66 [==============================] - 127s 2s/step - loss: 3.1833 - categorical_accuracy: 0.9232 - val_loss: 3.5603 - val_categorical_accuracy: 0.7563 - lr: 4.0000e-07\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.1879 - categorical_accuracy: 0.9161\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.76552\n",
      "\n",
      "Epoch 00005: val_categorical_accuracy did not improve from 0.76552\n",
      "66/66 [==============================] - 126s 2s/step - loss: 3.1879 - categorical_accuracy: 0.9161 - val_loss: 3.5572 - val_categorical_accuracy: 0.7655 - lr: 4.0000e-07\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.1836 - categorical_accuracy: 0.9222\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.76552\n",
      "\n",
      "Epoch 00006: val_categorical_accuracy did not improve from 0.76552\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.00000009348878e-08.\n",
      "66/66 [==============================] - 126s 2s/step - loss: 3.1836 - categorical_accuracy: 0.9222 - val_loss: 3.5557 - val_categorical_accuracy: 0.7655 - lr: 4.0000e-07\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.1967 - categorical_accuracy: 0.9132\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.76552\n",
      "\n",
      "Epoch 00007: val_categorical_accuracy did not improve from 0.76552\n",
      "66/66 [==============================] - 126s 2s/step - loss: 3.1967 - categorical_accuracy: 0.9132 - val_loss: 3.5539 - val_categorical_accuracy: 0.7632 - lr: 8.0000e-08\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.1913 - categorical_accuracy: 0.9118\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.76552\n",
      "\n",
      "Epoch 00008: val_categorical_accuracy did not improve from 0.76552\n",
      "66/66 [==============================] - 126s 2s/step - loss: 3.1913 - categorical_accuracy: 0.9118 - val_loss: 3.5563 - val_categorical_accuracy: 0.7632 - lr: 8.0000e-08\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.1826 - categorical_accuracy: 0.9132\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.76552\n",
      "\n",
      "Epoch 00009: val_categorical_accuracy did not improve from 0.76552\n",
      "66/66 [==============================] - 126s 2s/step - loss: 3.1826 - categorical_accuracy: 0.9132 - val_loss: 3.5560 - val_categorical_accuracy: 0.7655 - lr: 8.0000e-08\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - ETA: 0s - loss: 3.1788 - categorical_accuracy: 0.9227\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.76552\n",
      "\n",
      "Epoch 00010: val_categorical_accuracy did not improve from 0.76552\n",
      "66/66 [==============================] - 126s 2s/step - loss: 3.1788 - categorical_accuracy: 0.9227 - val_loss: 3.5553 - val_categorical_accuracy: 0.7655 - lr: 8.0000e-08\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAV7SZNYsgu1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxATIIPzsL7S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "01LWfMOWeiW2",
    "outputId": "b76bd32a-ff25-4c79-e207-46c4bd61d7f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 11s 722ms/step - loss: 3.4728 - categorical_accuracy: 0.7932\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.data.TFRecordDataset(os.path.join(data_path, 'test.tfrecords'), compression_type='GZIP').map(decode).batch(batch_size)\n",
    "results = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(test_dataset, model):\n",
    "# THis is for modify actual test labels\n",
    "    def df(t):\n",
    "        y =[]\n",
    "        for i in range(len(t)):\n",
    "            for j in range(len(t[i])):\n",
    "                max = np.argmax(t[i][j])\n",
    "                y.append(max)\n",
    "        return y\n",
    "    \n",
    "# THis is for modify predicted test labels    \n",
    "    def cf(x):\n",
    "        x = np.asarray(x)\n",
    "        y =[]\n",
    "        for i in range(len(x)):\n",
    "            max = np.argmax(x[i])\n",
    "            y.append(max)\n",
    "        return np.asarray(y)\n",
    "    \n",
    "# THis is for extract actual test labels    \n",
    "    t = []\n",
    "    for image, label in test_dataset:\n",
    "        t.append(label)\n",
    "\n",
    "# Predicted Test labels\n",
    "    predictions = model.predict(test_dataset)\n",
    "\n",
    "# Modified actual Test labels\n",
    "    y_test = df(t)\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "# Modified predicted test labels\n",
    "    y_pred = cf(predictions)\n",
    "    \n",
    "# Confusion Matrix\n",
    "    conf = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average = None)\n",
    "    recall = metrics.recall_score(y_test, y_pred, average= None)\n",
    "    \n",
    "    return accuracy, f1_score, recall\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "h6QLZdl7us-y",
    "outputId": "15f5f57e-f632-4349-acf8-e443462dc6dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.7931769722814499\n",
      "F1 Scores are [0.8914956  0.6993007  0.77170418]\n",
      "Recall Scores are [0.95597484 0.63694268 0.78431373]\n"
     ]
    }
   ],
   "source": [
    "print(\"The accuracy is \" + str(accuracy))\n",
    "print(\"F1 Scores are \" + str(f1_score))\n",
    "print(\"Recall Scores are \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BABXe3unxXER"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
