{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_leakyrelu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-moriGADdaj",
        "colab_type": "code",
        "outputId": "206eb08f-2a5c-4b46-dc51-b4553a45a4b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YBCrmoinmCl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %cd /content/drive/Shared drives/IML Project/Project\n",
        "data_path = '/content/drive/Shared drives/IML Project/Project/data/tfrecords_data'\n",
        "project_path = '/content/drive/Shared drives/IML Project/Project/baseline_LeakyReLu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zDXPAEwoLnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbN5skodoNzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv3D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import PReLU\n",
        "from tensorflow.keras.layers import MaxPool3D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import AveragePooling3D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZM1nchJoSdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "871hMI0noYb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode(serialized_example):\n",
        "    \"\"\"\n",
        "    Parses an image and label from the given `serialized_example`.\n",
        "    It is used as a map function for `dataset.map`\n",
        "    \"\"\"\n",
        "\n",
        "    features = tf.compat.v1.parse_single_example(\n",
        "        serialized_example,\n",
        "        features={\n",
        "            'img_channels': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
        "            'img_height': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
        "            'img_width': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
        "            'img_raw': tf.compat.v1.FixedLenFeature([], tf.string),\n",
        "            'sex': tf.compat.v1.FixedLenFeature([], tf.string),\n",
        "            'age': tf.compat.v1.FixedLenFeature([], tf.float32),\n",
        "            'label': tf.compat.v1.FixedLenFeature([], tf.string),\n",
        "        })\n",
        "    \n",
        "    \n",
        "    \n",
        "    height = tf.cast(features['img_height'], tf.int32)\n",
        "    width = tf.cast(features['img_width'], tf.int32)\n",
        "    channels = tf.cast(features['img_channels'], tf.int32)\n",
        "    image = tf.compat.v1.decode_raw(features['img_raw'], tf.float64)\n",
        "    image = tf.reshape(image, (channels, height, width, 1))\n",
        "    label = tf.compat.v1.decode_raw(features['label'], tf.float64)\n",
        "    sex = tf.cast(features['sex'], tf.string)\n",
        "    age = tf.cast(features['age'], tf.float32)\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7BUAjNIohmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.TFRecordDataset(os.path.join(data_path, 'train.tfrecords'), compression_type='GZIP').map(decode).batch(batch_size)\n",
        "val_dataset = tf.data.TFRecordDataset(os.path.join(data_path, 'validation.tfrecords'), compression_type='GZIP').map(decode).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpKfLlFcoq0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
        "    he_normal = tf.keras.initializers.he_normal(seed=0)\n",
        "    lecun_normal = tf.keras.initializers.lecun_normal(seed=0)\n",
        "\n",
        "    input_3d = (62, 96, 96, 1)\n",
        "    pool_3d = (2, 2, 2)\n",
        "\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(tf.keras.layers.InputLayer(input_shape=input_3d))\n",
        "    model.add(Conv3D(filters=8,\n",
        "                     kernel_size=3,\n",
        "                     kernel_regularizer=kernel_regularizer,\n",
        "                     kernel_initializer=he_normal))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(MaxPool3D(pool_size=pool_3d, name='pool1'))\n",
        "\n",
        "    model.add(Conv3D(filters=8,\n",
        "                     kernel_size=3,\n",
        "                     kernel_regularizer=kernel_regularizer,\n",
        "                     kernel_initializer=he_normal))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(MaxPool3D(pool_size=pool_3d, name='pool2'))\n",
        "\n",
        "    model.add(Conv3D(filters=8,\n",
        "                     kernel_size=3,\n",
        "                     kernel_regularizer=kernel_regularizer,\n",
        "                     kernel_initializer=he_normal))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(MaxPool3D(pool_size=pool_3d, name='pool3'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1024, name='dense1', kernel_regularizer=kernel_regularizer, kernel_initializer=he_normal))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.5, name='dropout1'))\n",
        "\n",
        "    model.add(Dense(512, activation='relu', name='dense2', kernel_regularizer=kernel_regularizer, kernel_initializer=he_normal))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.5, name='dropout2'))\n",
        "\n",
        "    model.add(Dense(3, activation='softmax', name='softmax', kernel_initializer=lecun_normal))\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "                  metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q2V_YlToxFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reducelr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy',\n",
        "                                                         factor=0.2,\n",
        "                                                         patience=5,\n",
        "                                                         min_delta=0.01,\n",
        "                                                         verbose=1)\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(os.path.join(project_path, 'checkpoints', 'cp.ckpt'),\n",
        "                                                         monitor='val_categorical_accuracy',\n",
        "                                                         verbose=1,\n",
        "                                                         save_best_only=True,\n",
        "                                                         mode='max')\n",
        "weights_callback = tf.keras.callbacks.ModelCheckpoint(os.path.join(project_path, 'weights', 'best_weights.h5'),\n",
        "                                                      monitor='val_categorical_accuracy',\n",
        "                                                      verbose=1,\n",
        "                                                      save_best_only=True,\n",
        "                                                      save_weights_only=True,\n",
        "                                                      mode='max')\n",
        "\n",
        "callbacks_list = [weights_callback, checkpoint_callback, reducelr_callback]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XceAcvCXo05H",
        "colab_type": "code",
        "outputId": "46ec3980-b340-4c16-af54-38ae0200d533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        }
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_3 (Conv3D)            (None, 60, 94, 94, 8)     224       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 60, 94, 94, 8)     32        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 60, 94, 94, 8)     0         \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling3D)         (None, 30, 47, 47, 8)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_4 (Conv3D)            (None, 28, 45, 45, 8)     1736      \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 45, 45, 8)     32        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 28, 45, 45, 8)     0         \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling3D)         (None, 14, 22, 22, 8)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_5 (Conv3D)            (None, 12, 20, 20, 8)     1736      \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 12, 20, 20, 8)     32        \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)    (None, 12, 20, 20, 8)     0         \n",
            "_________________________________________________________________\n",
            "pool3 (MaxPooling3D)         (None, 6, 10, 10, 8)      0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4800)              0         \n",
            "_________________________________________________________________\n",
            "dense1 (Dense)               (None, 1024)              4916224   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout1 (Dropout)           (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense2 (Dense)               (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout2 (Dropout)           (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "softmax (Dense)              (None, 3)                 1539      \n",
            "=================================================================\n",
            "Total params: 5,446,355\n",
            "Trainable params: 5,446,307\n",
            "Non-trainable params: 48\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU9OWXtbQ0gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.exists(os.path.join(project_path, 'checkpoints', 'cp.ckpt')):\n",
        "    model = load_model(os.path.join(project_path, 'checkpoints', 'cp.ckpt'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xwc9JhLo3k6",
        "colab_type": "code",
        "outputId": "0cf6dfc5-a826-4960-fce8-28c8a1e26017",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, callbacks=callbacks_list, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "     66/Unknown - 87s 1s/step - loss: 3.1834 - categorical_accuracy: 0.9175\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.75862, saving model to weights/best_weights.h5\n",
            "\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.75862, saving model to checkpoints/cp.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: checkpoints/cp.ckpt/assets\n",
            "66/66 [==============================] - 111s 2s/step - loss: 3.1834 - categorical_accuracy: 0.9175 - val_loss: 3.5632 - val_categorical_accuracy: 0.7586 - lr: 4.0000e-07\n",
            "Epoch 2/10\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.1806 - categorical_accuracy: 0.9260\n",
            "Epoch 00002: val_categorical_accuracy did not improve from 0.75862\n",
            "\n",
            "Epoch 00002: val_categorical_accuracy did not improve from 0.75862\n",
            "66/66 [==============================] - 127s 2s/step - loss: 3.1806 - categorical_accuracy: 0.9260 - val_loss: 3.5630 - val_categorical_accuracy: 0.7563 - lr: 4.0000e-07\n",
            "Epoch 3/10\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.1892 - categorical_accuracy: 0.9151\n",
            "Epoch 00003: val_categorical_accuracy improved from 0.75862 to 0.76782, saving model to weights/best_weights.h5\n",
            "\n",
            "Epoch 00003: val_categorical_accuracy improved from 0.75862 to 0.76782, saving model to checkpoints/cp.ckpt\n",
            "INFO:tensorflow:Assets written to: checkpoints/cp.ckpt/assets\n",
            "66/66 [==============================] - 131s 2s/step - loss: 3.1892 - categorical_accuracy: 0.9151 - val_loss: 3.5566 - val_categorical_accuracy: 0.7678 - lr: 4.0000e-07\n",
            "Epoch 4/10\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.1811 - categorical_accuracy: 0.9203\n",
            "Epoch 00004: val_categorical_accuracy did not improve from 0.76782\n",
            "\n",
            "Epoch 00004: val_categorical_accuracy did not improve from 0.76782\n",
            "66/66 [==============================] - 127s 2s/step - loss: 3.1811 - categorical_accuracy: 0.9203 - val_loss: 3.5658 - val_categorical_accuracy: 0.7540 - lr: 4.0000e-07\n",
            "Epoch 5/10\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.1840 - categorical_accuracy: 0.9199\n",
            "Epoch 00005: val_categorical_accuracy did not improve from 0.76782\n",
            "\n",
            "Epoch 00005: val_categorical_accuracy did not improve from 0.76782\n",
            "66/66 [==============================] - 126s 2s/step - loss: 3.1840 - categorical_accuracy: 0.9199 - val_loss: 3.5626 - val_categorical_accuracy: 0.7540 - lr: 4.0000e-07\n",
            "Epoch 6/10\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.1793 - categorical_accuracy: 0.9184\n",
            "Epoch 00006: val_categorical_accuracy did not improve from 0.76782\n",
            "\n",
            "Epoch 00006: val_categorical_accuracy did not improve from 0.76782\n",
            "\n",
            "Epoch 00006: ReduceLROnPlateau reducing learning rate to 8.00000009348878e-08.\n",
            "66/66 [==============================] - 126s 2s/step - loss: 3.1793 - categorical_accuracy: 0.9184 - val_loss: 3.5612 - val_categorical_accuracy: 0.7609 - lr: 4.0000e-07\n",
            "Epoch 7/10\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.1798 - categorical_accuracy: 0.9170\n",
            "Epoch 00007: val_categorical_accuracy did not improve from 0.76782\n",
            "\n",
            "Epoch 00007: val_categorical_accuracy did not improve from 0.76782\n",
            "66/66 [==============================] - 126s 2s/step - loss: 3.1798 - categorical_accuracy: 0.9170 - val_loss: 3.5599 - val_categorical_accuracy: 0.7655 - lr: 8.0000e-08\n",
            "Epoch 8/10\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.1727 - categorical_accuracy: 0.9275\n",
            "Epoch 00008: val_categorical_accuracy did not improve from 0.76782\n",
            "\n",
            "Epoch 00008: val_categorical_accuracy did not improve from 0.76782\n",
            "66/66 [==============================] - 126s 2s/step - loss: 3.1727 - categorical_accuracy: 0.9275 - val_loss: 3.5598 - val_categorical_accuracy: 0.7655 - lr: 8.0000e-08\n",
            "Epoch 9/10\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.1983 - categorical_accuracy: 0.9052\n",
            "Epoch 00009: val_categorical_accuracy did not improve from 0.76782\n",
            "\n",
            "Epoch 00009: val_categorical_accuracy did not improve from 0.76782\n",
            "66/66 [==============================] - 126s 2s/step - loss: 3.1983 - categorical_accuracy: 0.9052 - val_loss: 3.5612 - val_categorical_accuracy: 0.7609 - lr: 8.0000e-08\n",
            "Epoch 10/10\n",
            "66/66 [==============================] - ETA: 0s - loss: 3.1764 - categorical_accuracy: 0.9208\n",
            "Epoch 00010: val_categorical_accuracy did not improve from 0.76782\n",
            "\n",
            "Epoch 00010: val_categorical_accuracy did not improve from 0.76782\n",
            "66/66 [==============================] - 127s 2s/step - loss: 3.1764 - categorical_accuracy: 0.9208 - val_loss: 3.5621 - val_categorical_accuracy: 0.7563 - lr: 8.0000e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01LWfMOWeiW2",
        "colab_type": "code",
        "outputId": "6458c157-ca89-40df-a6f1-39df73cd4f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_dataset = tf.data.TFRecordDataset(os.path.join(data_path, 'test.tfrecords'), compression_type='GZIP').map(decode).shuffle(100000).batch(batch_size)\n",
        "results = model.evaluate(test_dataset)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15/15 [==============================] - 16s 1s/step - loss: 3.4720 - categorical_accuracy: 0.7932\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFKMo8e1EXAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8obWmRO_DtmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stats2(model):\n",
        "   \n",
        "# THis is for modify predicted test labels    \n",
        "  def cf(x):\n",
        "      x = np.asarray(x)\n",
        "      y =[]\n",
        "      for i in range(len(x)):\n",
        "          max = np.argmax(x[i])\n",
        "          y.append(max)\n",
        "      return np.asarray(y)\n",
        "  \n",
        "  def load_test_dataset():\n",
        "      i = 1\n",
        "      data_path = '/content/drive/Shared drives/IML Project/Project/data/numpy_data'\n",
        "      dataset = np.load(os.path.join(data_path, f'img_array_test_6k_{i}.npy'))\n",
        "      while True:\n",
        "          try:\n",
        "              i += 1\n",
        "              dataset = np.vstack((dataset, np.load(os.path.join(data_path, f'img_array_test_6k_{i}.npy'))))\n",
        "          except FileNotFoundError:\n",
        "              print(f'Loaded all test datasets')\n",
        "              break\n",
        "      # dataset = np.expand_dims(dataset, axis=1)\n",
        "      for n in range(dataset.shape[0]):\n",
        "          dataset[n, :, :] = dataset[n, :, :] / np.amax(dataset[n, :, :].flatten())\n",
        "      print(f'Normalized {n+1} images')\n",
        "      dataset = np.reshape(dataset, (-1, 62, 96, 96, 1))\n",
        "      return dataset, np.eye(3)[labels[labels.train_valid_test == 2]['diagnosis'] - 1]\n",
        "\n",
        "  labels = pd.read_csv('/content/drive/Shared drives/IML Project/Project/demographics.csv')\n",
        "  test_data, test_labels = load_test_dataset()\n",
        "\n",
        "  # Predicted Test labels\n",
        "  predictions = model.predict(test_data)\n",
        "\n",
        "  # Modified actual Test labels\n",
        "  y_test = cf(test_labels)\n",
        "\n",
        "\n",
        "  # Modified predicted test labels\n",
        "  y_pred = cf(predictions)\n",
        "\n",
        "  # Confusion Matrix\n",
        "  conf = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "  f1_score = metrics.f1_score(y_test, y_pred, average = None)\n",
        "  recall = metrics.recall_score(y_test, y_pred, average= None)\n",
        "\n",
        "  return accuracy, f1_score, recall\n",
        "\n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO_kPPnaLhAy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "264de75e-2457-4186-cdbe-d8b793d39017"
      },
      "source": [
        "accuracy, f1_score, recall = stats2(model)\n",
        "print(\"The accuracy is \" + str(accuracy))\n",
        "print(\"F1 Scores are \" + str(f1_score))\n",
        "print(\"Recall Scores are \" + str(recall))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded all test datasets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Normalized 29078 images\n",
            "The accuracy is 0.7931769722814499\n",
            "F1 Scores are [0.89411765 0.6971831  0.77070064]\n",
            "Recall Scores are [0.95597484 0.63057325 0.79084967]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRHeZEGxLcoM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DU1ZxamMEcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def stats(test_dataset, model):\n",
        "# # THis is for modify actual test labels\n",
        "#     def df(t):\n",
        "#         y =[]\n",
        "#         for i in range(len(t)):\n",
        "#             for j in range(len(t[i])):\n",
        "#                 max = np.argmax(t[i][j])\n",
        "#                 y.append(max)\n",
        "#         return np.asarray(y)\n",
        "    \n",
        "# # THis is for modify predicted test labels    \n",
        "#     def cf(x):\n",
        "#         x = np.asarray(x)\n",
        "#         y =[]\n",
        "#         for i in range(len(x)):\n",
        "#             max = np.argmax(x[i])\n",
        "#             y.append(max)\n",
        "#         return np.asarray(y)\n",
        "    \n",
        "# # THis is for extract actual test labels    \n",
        "#     t = []\n",
        "#     for image, label in test_dataset:\n",
        "#         t.append(label)\n",
        "\n",
        "# # Predicted Test labels\n",
        "#     predictions = model.predict(test_dataset)\n",
        "\n",
        "# # Modified actual Test labels\n",
        "#     y_test = df(t)\n",
        "  \n",
        "    \n",
        "# # Modified predicted test labels\n",
        "#     y_pred = cf(predictions)\n",
        "    \n",
        "# # Confusion Matrix\n",
        "#     conf = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "#     accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "#     f1_score = metrics.f1_score(y_test, y_pred, average = None)\n",
        "#     recall = metrics.recall_score(y_test, y_pred, average= None)\n",
        "    \n",
        "#     return accuracy, f1_score, recall\n",
        "\n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}