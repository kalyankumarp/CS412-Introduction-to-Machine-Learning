{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-moriGADdaj"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-YBCrmoinmCl",
    "outputId": "5b6ef42e-09d5-41ab-e642-fe06c3a830cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/IML PRoject\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/IML PRoject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zDXPAEwoLnB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fbN5skodoNzY"
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.layers import MaxPool3D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import AveragePooling3D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import UpSampling3D\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZM1nchJoSdP"
   },
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\2nd Sem\\Courses\\CS 412 IML\\Project\\Alzheimers\\Implementations\\Our Implementations\\!Pure Deep Learning\\2\\tf_datasets'\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "871hMI0noYb5"
   },
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "    \"\"\"\n",
    "    Parses an image and label from the given `serialized_example`.\n",
    "    It is used as a map function for `dataset.map`\n",
    "    \"\"\"\n",
    "\n",
    "    features = tf.compat.v1.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'img_channels': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
    "            'img_height': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
    "            'img_width': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
    "            'img_raw': tf.compat.v1.FixedLenFeature([], tf.string),\n",
    "            'sex': tf.compat.v1.FixedLenFeature([], tf.string),\n",
    "            'age': tf.compat.v1.FixedLenFeature([], tf.float32),\n",
    "            'label': tf.compat.v1.FixedLenFeature([], tf.string),\n",
    "        })\n",
    "    \n",
    "    \n",
    "    \n",
    "    height = tf.cast(features['img_height'], tf.int32)\n",
    "    width = tf.cast(features['img_width'], tf.int32)\n",
    "    channels = tf.cast(features['img_channels'], tf.int32)\n",
    "    image = tf.compat.v1.decode_raw(features['img_raw'], tf.float64)\n",
    "    image = tf.reshape(image, (channels, height, width, 1))\n",
    "    label = tf.compat.v1.decode_raw(features['label'], tf.float64)\n",
    "    sex = tf.cast(features['sex'], tf.string)\n",
    "    age = tf.cast(features['age'], tf.float32)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7BUAjNIohmJ"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TFRecordDataset(os.path.join(data_path, 'train.tfrecords'), compression_type='GZIP').map(decode).batch(batch_size)\n",
    "val_dataset = tf.data.TFRecordDataset(os.path.join(data_path, 'validation.tfrecords'), compression_type='GZIP').map(decode).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWIdJd1WUkzt"
   },
   "outputs": [],
   "source": [
    "# class VariationalConvNetAutoEncoder(tf.keras.Model):\n",
    "#     def __init__(self):\n",
    "#         super(VariationalConvNetAutoEncoder, self).__init__()\n",
    "#         self.input_3d = (62, 96, 96, 1)\n",
    "#         self.pool_3d = (5, 5, 5)\n",
    "\n",
    "#         self.model = Sequential()\n",
    "        \n",
    "#         self.model.add(tf.keras.layers.InputLayer(input_shape=self.input_3d))\n",
    "#         self.conv1 = Conv3D(16, 3, activation='relu', padding='same')\n",
    "#         self.maxp1 = MaxPool3D(self.pool_3d, padding='same')\n",
    "#         self.conv2 = Conv3D(8, 3, activation='relu', padding='same')\n",
    "#         self.maxp2 = MaxPool3D(self.pool_3d, padding='same')\n",
    "#         self.conv3 = Conv3D(8, 3, activation='relu', padding='same')\n",
    "        \n",
    "#         # self.mu = MaxPool3D(self.pool_3d, padding='same')\n",
    "#         # self.sigma = MaxPool3D(self.pool_3d, padding='same')\n",
    "        \n",
    "#         self.conv4 = Conv3D(8, 3, activation='relu', padding='same')\n",
    "#         self.upsample1 = UpSampling3D(self.pool_3d)\n",
    "#         self.conv5 = Conv3D(8, 3, activation='relu', padding='same')\n",
    "#         self.upsample2 = UpSampling3D(self.pool_3d)\n",
    "#         self.conv6 = Conv3D(16, 3, activation='relu')\n",
    "#         self.upsample3 = UpSampling3D(self.pool_3d)\n",
    "#         self.conv7 = Conv3D(1, 3, activation='sigmoid', padding='same')\n",
    "        \n",
    "#     def encoder(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.maxp1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.maxp2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         mu = self.mu(x)\n",
    "#         sigma = self.sigma(x)\n",
    "#         return mu, sigma\n",
    "    \n",
    "#     def decoder(self, x):\n",
    "#         x = self.conv4(x)\n",
    "#         x = self.upsample1(x)\n",
    "#         x = self.conv5(x)\n",
    "#         x = self.upsample2(x)\n",
    "#         x = self.conv6(x)\n",
    "#         x = self.upsample3(x)\n",
    "#         x = self.conv7(x)\n",
    "#         return x\n",
    "    \n",
    "#     def sample_from_mu_sigma(self, mu, sigma):\n",
    "#         std = tf.exp(0.5 * sigma)\n",
    "#         eps = tf.random_normal(shape=std.shape)\n",
    "#         return mu + eps * std\n",
    "        \n",
    "#     def call(self, x):\n",
    "#         mu, sigma = self.encoder(x)\n",
    "#         z = self.sample_from_mu_sigma(mu, sigma)\n",
    "#         y = self.decoder(z)\n",
    "#         return y, mu, sigma\n",
    "        \n",
    "# def loss(x, x_bar, mu, sigma):\n",
    "#   return tf.losses.mean_squared_error(x, x_bar) - (0.5 * tf.reduce_sum(1 + sigma - tf.math.pow(mu, 2) - tf.exp(sigma)))\n",
    "# def grad(model, inputs, targets):\n",
    "#   with tf.GradientTape() as tape:\n",
    "#     reconstruction, mu, sigma = model(inputs)\n",
    "#     loss_value = loss(targets, reconstruction, mu, sigma)\n",
    "#   return loss_value, tape.gradient(loss_value, model.trainable_variables), reconstruction, mu, sigma\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encoder(model):\n",
    "#     model.add(Conv3D(filters=16,\n",
    "#                      kernel_size=2,\n",
    "#                      kernel_regularizer=kernel_regularizer,\n",
    "#                      kernel_initializer=he_normal))\n",
    "#     model.add(LeakyReLU())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool3D(pool_size=pool_3d, name='pool1'))\n",
    "\n",
    "#     model.add(Conv3D(filters=8,\n",
    "#                      kernel_size=2,\n",
    "#                      kernel_regularizer=kernel_regularizer,\n",
    "#                      kernel_initializer=he_normal))\n",
    "#     model.add(LeakyReLU())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool3D(pool_size=pool_3d, name='pool2'))\n",
    "\n",
    "#     model.add(Conv3D(filters=8,\n",
    "#                      kernel_size=2,\n",
    "#                      kernel_regularizer=kernel_regularizer,\n",
    "#                      kernel_initializer=he_normal))\n",
    "#     model.add(LeakyReLU())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(MaxPool3D(pool_size=pool_3d, name='pool3'))\n",
    "#     return model\n",
    "\n",
    "# def decoder(model):\n",
    "#     model.add(Conv3D(filters=8,\n",
    "#                      kernel_size=2,\n",
    "#                      kernel_regularizer=kernel_regularizer,\n",
    "#                      kernel_initializer=he_normal))\n",
    "#     model.add(LeakyReLU())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(UpSampling3D(pool_3d, name='Up1'))\n",
    "\n",
    "#     model.add(Conv3D(filters=16,\n",
    "#                      kernel_size=2,\n",
    "#                      kernel_regularizer=kernel_regularizer,\n",
    "#                      kernel_initializer=he_normal))\n",
    "#     model.add(LeakyReLU())\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(UpSampling3D(pool_3d, name='Up2'))\n",
    "\n",
    "#     model.add(Conv3D(filters=1,\n",
    "#                      kernel_size=2,\n",
    "#                      kernel_regularizer=kernel_regularizer,\n",
    "#                      kernel_initializer=he_normal, activation='sigmoid'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\2nd Sem\\Courses\\CS 412 IML\\Project\\Alzheimers\\Implementations\\Our Implementations\\!Pure Deep Learning\\2\\demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(type: str):\n",
    "    if type not in ['train', 'test', 'valid']: raise Exception('Unsupported dataset type')\n",
    "    train_valid_test = 0 if type == 'train' else 1 if type == 'valid' else 2\n",
    "    i = 1\n",
    "    data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\2nd Sem\\Courses\\CS 412 IML\\Project\\Alzheimers\\Implementations\\Our Implementations\\!Pure Deep Learning\\2\\numpy_data'\n",
    "    dataset = np.load(os.path.join(data_path, f'img_array_{type}_6k_{i}.npy'))\n",
    "    while True:\n",
    "        try:\n",
    "            i += 1\n",
    "            dataset = np.vstack((dataset, np.load(os.path.join(data_path, f'img_array_{type}_6k_{i}.npy'))))\n",
    "        except FileNotFoundError:\n",
    "            print(f'Loaded all {type} datasets')\n",
    "            break\n",
    "    # dataset = np.expand_dims(dataset, axis=1)\n",
    "    for n in range(dataset.shape[0]):\n",
    "        dataset[n, :, :] = dataset[n, :, :] / np.amax(dataset[n, :, :].flatten())\n",
    "    print(f'Normalized {n+1} images')\n",
    "    dataset = np.reshape(dataset, (-1, 62, 96, 96, 1))\n",
    "    return dataset, labels[labels.train_valid_test == train_valid_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all valid datasets\n",
      "Normalized 26970 images\n"
     ]
    }
   ],
   "source": [
    "valid_data, valid_demo = load_datasets('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[435,31,48,48,32] and type double on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:AddV2] name: batch_normalization_49/batchnorm/add/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-cffa770b27b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchanDim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# flatten the network and then construct our latent vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\normalization.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    835\u001b[0m                                      \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m                                      \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 837\u001b[1;33m                                      self.epsilon)\n\u001b[0m\u001b[0;32m    838\u001b[0m     \u001b[1;31m# If some components of the shape got lost due to adjustments, fix that.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py\u001b[0m in \u001b[0;36mbatch_normalization\u001b[1;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;31m# the precise order of ops that are generated by the expression below.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m     return x * math_ops.cast(inv, x.dtype) + math_ops.cast(\n\u001b[1;32m-> 1434\u001b[1;33m         offset - mean * inv if offset is not None else -mean * inv, x.dtype)\n\u001b[0m\u001b[0;32m   1435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 902\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    903\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6605\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6606\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6607\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[435,31,48,48,32] and type double on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu [Op:AddV2] name: batch_normalization_49/batchnorm/add/"
     ]
    }
   ],
   "source": [
    "filters=[32, 64]\n",
    "latentDim=16\n",
    "chanDim = -1\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "# define the input to the encoder\n",
    "inputs = valid_data\n",
    "x = inputs\n",
    "\n",
    "# loop over the number of filters\n",
    "for f in filters:\n",
    "    # apply a CONV => RELU => BN operation\n",
    "    x = Conv3D(f, 3, strides=2, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "# flatten the network and then construct our latent vector\n",
    "volumeSize = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "latent = Dense(latentDim)(x)\n",
    "\n",
    "# build the encoder model\n",
    "encoder = Model(inputs, latent, name=\"encoder\")\n",
    "\n",
    "# start building the decoder model which will accept the\n",
    "# output of the encoder as its inputs\n",
    "latentInputs = Input(shape=(latentDim,))\n",
    "x = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
    "x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "\n",
    "# loop over our number of filters again, but this time in\n",
    "# reverse order\n",
    "for f in filters[::-1]:\n",
    "    # apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "    x = Conv3DTranspose(f, 3, strides=2, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "# apply a single CONV_TRANSPOSE layer used to recover the\n",
    "# original depth of the image\n",
    "x = Conv3DTranspose(1, 3, padding=\"same\")(x)\n",
    "outputs = Activation(\"sigmoid\")(x)\n",
    "\n",
    "# build the decoder model\n",
    "decoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "\n",
    "# our autoencoder is the encoder + decoder\n",
    "autoencoder = Model(inputs, decoder(encoder(inputs)),\n",
    "    name=\"autoencoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "liAON0_vUqOi"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Model' has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-07a6f44812f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_3d\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m model.compile(loss='categorical_crossentropy',\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'Model' has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "\n",
    "kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "he_normal = tf.keras.initializers.he_normal(seed=0)\n",
    "lecun_normal = tf.keras.initializers.lecun_normal(seed=0)\n",
    "\n",
    "input_3d = (62, 96, 96, 1)\n",
    "pool_3d = (2, 2, 2)\n",
    "\n",
    "from tensorflow.keras.models import Model as model\n",
    "\n",
    "model.add(tf.keras.layers.InputLayer(input_shape=input_3d))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "encoded_data = encoder(model).fit(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpKfLlFcoq0e"
   },
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "#     kernel_regularizer = tf.keras.regularizers.l2(0.001)\n",
    "#     he_normal = tf.keras.initializers.he_normal(seed=0)\n",
    "#     lecun_normal = tf.keras.initializers.lecun_normal(seed=0)\n",
    "\n",
    "#     input_3d = (62, 96, 96, 1)\n",
    "#     pool_3d = (5, 5, 5)\n",
    "\n",
    "#     model = Sequential()\n",
    "    \n",
    "#     model.add(tf.keras.layers.InputLayer(input_shape=input_3d))\n",
    "#     model.add(Conv3D(filters=8,\n",
    "#                      kernel_size=5,\n",
    "#                      kernel_regularizer=kernel_regularizer,\n",
    "#                      kernel_initializer=he_normal))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(LeakyReLU())\n",
    "#     model.add(MaxPool3D(pool_size=pool_3d, name='pool1'))\n",
    "\n",
    "#     model.add(Conv3D(filters=8,\n",
    "#                      kernel_size=5,\n",
    "#                      kernel_regularizer=kernel_regularizer,\n",
    "#                      kernel_initializer=he_normal))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(LeakyReLU())\n",
    "#     model.add(MaxPool3D(pool_size=pool_3d, name='pool2'))\n",
    "\n",
    "#     # model.add(Conv3D(filters=8,\n",
    "#     #                  kernel_size=2,\n",
    "#     #                  kernel_regularizer=kernel_regularizer,\n",
    "#     #                  kernel_initializer=he_normal))\n",
    "#     # model.add(BatchNormalization())\n",
    "#     # model.add(LeakyReLU())\n",
    "#     # model.add(MaxPool3D(pool_size=pool_3d, name='pool3'))\n",
    "\n",
    "#     # model.add(Conv3D(filters=8,\n",
    "#     #                  kernel_size=5,\n",
    "#     #                  kernel_regularizer=kernel_regularizer,\n",
    "#     #                  kernel_initializer=he_normal))\n",
    "#     # model.add(BatchNormalization())\n",
    "#     # model.add(LeakyReLU())\n",
    "#     # model.add(MaxPool3D(pool_size=pool_3d, name='pool4'))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(1024, name='dense1', kernel_regularizer=kernel_regularizer, kernel_initializer=he_normal))\n",
    "#     model.add(LeakyReLU())\n",
    "#     model.add(Dropout(0.5, name='dropout1'))\n",
    "\n",
    "#     model.add(Dense(512, activation='relu', name='dense2', kernel_regularizer=kernel_regularizer, kernel_initializer=he_normal))\n",
    "#     model.add(LeakyReLU())\n",
    "#     model.add(Dropout(0.5, name='dropout2'))\n",
    "\n",
    "#     model.add(Dense(3, activation='softmax', name='softmax', kernel_initializer=lecun_normal))\n",
    "\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#                   optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "#                   metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6q2V_YlToxFg"
   },
   "outputs": [],
   "source": [
    "reducelr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_categorical_accuracy',\n",
    "                                                         factor=0.2,\n",
    "                                                         patience=5,\n",
    "                                                         min_delta=0.01,\n",
    "                                                         verbose=1)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(os.path.join('checkpoints', 'cp.ckpt'),\n",
    "                                                         monitor='val_categorical_accuracy',\n",
    "                                                         verbose=1,\n",
    "                                                         save_best_only=True,\n",
    "                                                         mode='max')\n",
    "weights_callback = tf.keras.callbacks.ModelCheckpoint(os.path.join('weights', 'best_weights.h5'),\n",
    "                                                      monitor='val_categorical_accuracy',\n",
    "                                                      verbose=1,\n",
    "                                                      save_best_only=True,\n",
    "                                                      save_weights_only=True,\n",
    "                                                      mode='max')\n",
    "\n",
    "callbacks_list = [weights_callback, checkpoint_callback, reducelr_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "id": "XceAcvCXo05H",
    "outputId": "79c0d80e-e717-4042-e9be-ab25e3ba6ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_35 (Conv3D)           (None, 61, 95, 95, 16)    144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 61, 95, 95, 16)    64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 61, 95, 95, 16)    0         \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling3D)         (None, 30, 47, 47, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_36 (Conv3D)           (None, 29, 46, 46, 8)     1032      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 29, 46, 46, 8)     32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_30 (LeakyReLU)   (None, 29, 46, 46, 8)     0         \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling3D)         (None, 14, 23, 23, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_37 (Conv3D)           (None, 13, 22, 22, 8)     520       \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 13, 22, 22, 8)     32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_31 (LeakyReLU)   (None, 13, 22, 22, 8)     0         \n",
      "_________________________________________________________________\n",
      "pool3 (MaxPooling3D)         (None, 6, 11, 11, 8)      0         \n",
      "_________________________________________________________________\n",
      "conv3d_38 (Conv3D)           (None, 5, 10, 10, 8)      520       \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 5, 10, 10, 8)      32        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 5, 10, 10, 8)      0         \n",
      "_________________________________________________________________\n",
      "Up1 (UpSampling3D)           (None, 10, 20, 20, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_39 (Conv3D)           (None, 9, 19, 19, 16)     1040      \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 9, 19, 19, 16)     64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 9, 19, 19, 16)     0         \n",
      "_________________________________________________________________\n",
      "Up2 (UpSampling3D)           (None, 18, 38, 38, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_40 (Conv3D)           (None, 17, 37, 37, 1)     129       \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 17, 37, 37, 1)     4         \n",
      "=================================================================\n",
      "Total params: 3,613\n",
      "Trainable params: 3,499\n",
      "Non-trainable params: 114\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_AE()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CU9OWXtbQ0gH"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join('checkpoints', 'cp.ckpt')):\n",
    "    model = load_model(os.path.join('checkpoints', 'cp.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCPmQA4neabp"
   },
   "outputs": [],
   "source": [
    "# 8.0000e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "0Xwc9JhLo3k6",
    "outputId": "a0ea9fbd-0452-4a2b-89b2-b80f248a6704"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "      1/Unknown - 0s 22ms/stepWARNING:tensorflow:Can save best model only with val_categorical_accuracy available, skipping.\n",
      "WARNING:tensorflow:Can save best model only with val_categorical_accuracy available, skipping.\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_categorical_accuracy` which is not available. Available metrics are: lr\n",
      "      1/Unknown - 0s 25ms/step"
     ]
    },
    {
     "ename": "DataLossError",
     "evalue": " inflate() failed with error -3: incorrect header check\n\t [[node IteratorGetNext (defined at <ipython-input-49-0e7deba41ea6>:1) ]] [Op:__inference_distributed_function_8647]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-8ef5abc167d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mDataLossError\u001b[0m:  inflate() failed with error -3: incorrect header check\n\t [[node IteratorGetNext (defined at <ipython-input-49-0e7deba41ea6>:1) ]] [Op:__inference_distributed_function_8647]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, validation_data=train_dataset, epochs=40, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAV7SZNYsgu1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SxATIIPzsL7S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "01LWfMOWeiW2",
    "outputId": "ab169ffe-ce60-4644-983b-db3ce3a7a692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 14s 902ms/step - loss: 4.0485 - categorical_accuracy: 0.4819\n"
     ]
    }
   ],
   "source": [
    "test_dataset = tf.data.TFRecordDataset(os.path.join(data_path, 'test.tfrecords'), compression_type='GZIP').map(decode).batch(batch_size)\n",
    "results = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ufVV0G961yuG"
   },
   "outputs": [],
   "source": [
    "t = []\n",
    "for image, label in test_dataset:\n",
    "  t.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g1utHpKw2d8W",
    "outputId": "bb57b6b1-6abe-4ce3-8e78-c1034aa9a860"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ealgOdXe178X"
   },
   "outputs": [],
   "source": [
    "def df(t):\n",
    "  y =[]\n",
    "  for i in range(len(t)):\n",
    "    for j in range(len(t[i])):\n",
    "          max = np.argmax(t[i][j])\n",
    "          y.append(max)\n",
    "  return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cLJlrW1hyYek"
   },
   "outputs": [],
   "source": [
    "y_test = df(t)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NvC9Sfzswob2"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "qEITK017ug2B",
    "outputId": "5ec81707-0c37-4acd-fa2c-0e28ccfef8d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00653354, 0.11199015, 0.8814763 ],\n",
       "       [0.9644184 , 0.02377447, 0.01180711],\n",
       "       [0.00872518, 0.014667  , 0.9766078 ],\n",
       "       ...,\n",
       "       [0.31045234, 0.35707214, 0.3324755 ],\n",
       "       [0.19253181, 0.7281903 , 0.07927796],\n",
       "       [0.06457602, 0.07411502, 0.86130893]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56kAPEABumTh"
   },
   "outputs": [],
   "source": [
    "def cf(x):\n",
    "    x = np.asarray(x)\n",
    "    y =[]\n",
    "    for i in range(len(x)):\n",
    "        max = np.argmax(x[i])\n",
    "        y.append(max)\n",
    "    return np.asarray(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rj03Qbzsx5RL"
   },
   "outputs": [],
   "source": [
    "# y_test =np.asarray([i for i in list(test_labels)])\n",
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDNPmSSMupAd"
   },
   "outputs": [],
   "source": [
    "y_pred = cf(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CQFQ8pc1XVy"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "NM_DvDLTuq9v",
    "outputId": "050b6361-ff15-4104-b1e7-4c2fea44fd26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[152,   3,   4],\n",
       "       [ 23, 100,  34],\n",
       "       [  7,  26, 120]])"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = metrics.confusion_matrix(y_test, y_pred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "h6QLZdl7us-y",
    "outputId": "15f5f57e-f632-4349-acf8-e443462dc6dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.7931769722814499\n",
      "F1 Scores are [0.8914956  0.6993007  0.77170418]\n",
      "Recall Scores are [0.95597484 0.63694268 0.78431373]\n"
     ]
    }
   ],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average = None)\n",
    "recall = metrics.recall_score(y_test, y_pred, average= None)\n",
    "print(\"The accuracy is \" + str(accuracy))\n",
    "print(\"F1 Scores are \" + str(f1_score))\n",
    "print(\"Recall Scores are \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BABXe3unxXER"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
